---
title: "TITLE PROJECT 2"
author: "Team 3; Group 1"
date: "02/09/2021"
output:
  html_document:
    df_print: paged
    number_sections: yes
---
```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.pos = 'H')
```

***

# Abstract 

Alcohol-related traffic fatalities are a serious issue in the United States, considered a public health crisis by some (Perrine, Peck, Fell). Studies have shown that the decisions of policy makers to implement laws to protect their citizens from alcohol-related traffic accidents have correlated with improvement in such accidents (Lovenheim, Slemrod; Mast, Benson, Rasmussen). This study assesses the relationship between various state laws, including alcohol tax, legal drinking age, and punishments for driving-wile-intoxicated, on alcohol fatality rates. WHAT METHODS WE USED (1 SENTENCE). WHAT WERE THE RESULTS (2 SENTENCES).

# Introduction

Traffic fatalities related to alcohol consumption is a big problem in the United States. Over 10,000 people died from crashes due to driving while intoxicated in 2018, comprising nearly 30% of traffic fatalities that year. Some have likened the impact of these deaths to a public health, or epidemiological, problem given the complex social and cultural circumstances that surround this problem.  (https://www.nhtsa.gov/risky-driving/drunk-driving cite this last two sentences). Public representatives have implemented several different types of laws to help combat this problem. These laws can be broadly defined as "decreasing opportunity", or "preventing recidivism". In other words, these laws either: 1) make it less convenient (and theoretically, less probable) that one individual would drive drunk, or 2) make the penalty so large for a first offense that a previous offender will correct their behavior the next time. Successful deterrence of driving-while-intoxicated has the potential to save the lives of many citizens. 

In the 1980s, faced with the stark statistics, lawmakers implemented various laws related to, among other things, drinking age, beer tax and drunk driving penalties (cite). These laws were accumulated and described in a data set that will be discussed in greater detail later in this report. (cite our data set) The goal of this report is to study the efficacy of these law changes, across the United States, and over the course of the 1980s. In particular:
- Are any law change (particularly beer tax, penalization for drunk driving, or drinking age) correlated with the improvement in alcohol-related traffic fatalities per capita?
- Which combination of drunk-driving related variables in the data set best predict the rate of alcohol-related traffic fatalities?
- Is it possible to imply causality between these changes and the decrease or increase of alcohol-related traffic fatalities?

Based on previous literature, we hypothesize that lower alcohol-related traffic fatality rates will be correlated to a higher drinking age, higher beer tax, and more-stringent sentancing for drunk driving offenses. (cite ALSO ADD?) Better understanding the answers to these questions could help lawmakers assess the efficacy of drunk driving-related legislation on alcohol-related traffic fatalities. 
 
# Background 

The source of data used in this experiment is from a study conducted *then* by *them* to look at alcohol-related laws and their impact on drunk driving. This set is available through AER as the "Fatalities" data set. The target population of the original study was all 50 states in the USA, during the 1980s. The population is reported from each state, along with statistics including total traffic fatalities, alcohol-related traffic fatalities, and subsets of those variables separated by age. The "sampling" mechanism was reported accidents and fatalities available in the public domain. While some accidents (minor, on private property, or no injuries for instance) may not have been reported, it is relatively safe to presume that all instance were reported. Totals (for counts) or means (for values) associated with each variable were aggregated on a yearly basis by state. The variables used in this study include:FILL IN ONCE WE KNOW.

# Descriptive analysis 

The rate of alcohol-related drinking fatalities fluctuated by state over the 7 years included in this study. It appears that alcohol related traffic fatalities decreases in most states over most year.

![](full_us.gif)

Preliminary analysis suggests that law-changes were similar between the southeastern-to-southcentral region and the northeaster-to-midatlantic region. 

![](south.gif)

![](northeast.gif)

## Drinking Age
Based on the literature, drinking age seems to be a state's first-line of defense against alcohol-related traffic fatalities. Preliminary data exploration shows that by far, the most common drinking age is 21. Drinking age is significantly related to the rate of alcohol-related traffic fatalities (p = ). State, however, also appears to be significant. This is telling because it suggests that other state laws are significantly affecting drunk driving. Because the drinking age is now 21 across the nation, this variable does not contribute actionable information to this study. The other state laws may still be actionable and warrant further study.

**Insert table with the, like, 6 ANOVA results.

# Inferential analysis 

##depiction of the models we use in statistical terms

### Model Fit
```{r, echo = FALSE, message=FALSE, results = 'hide'}
library(ggplot2)
library(viridis)
library(AER)
library(MASS)
data(Fatalities)
dim(Fatalities)

Fatalities$afatal_rate <- Fatalities$afatal / Fatalities$pop * 10000
## different us regions
south = c("la","ms","al","ga","sc","ar","tx","fl","nc","tn","ar","ok","ky","wv","va")
n_east =  c("me","vt","nh","ma","ny","ri","ct","nj","de","md","pa")

Fatalities$location[Fatalities$state %in% south]  =  "south"
Fatalities$location[Fatalities$state %in% n_east] =  "north east"
Fatalities$location[!(Fatalities$state %in% n_east | Fatalities$state %in% south)]   =  "other"
Fatalities$location = as.factor(Fatalities$location)

Fatalities = Fatalities[c("afatal_rate","year", "spirits", "unemp", "income", "emppop", "beertax",    "baptist", "mormon","drinkage","dry","youngdrivers","miles","breath","jail", "service", "milestot", "unempus", "emppopus","gsp", "location")]

#remove the missing value:
Fatalities = Fatalities[-c(which(is.na(Fatalities$jail))),]

fatality.1982 = subset(Fatalities, year == "1982")
fatality.1982$year = NULL
fatality.1985 = subset(Fatalities, year == "1985")
fatality.1985$year = NULL
fatality.1988 = subset(Fatalities, year == "1988")
fatality.1988$year = NULL
```
####  Model Selection\
In this section we are using stepwise regression for all subsets and a specific subset selection process to see which combination of explanatory variables will produce the best model based off of the criteria we have chosen to use which is BIC. There are many variables in our model and we must make sure the ones in our model are significant. We looked at the model chosen when using all subsets and then Forwards Backwards Selection.

####  Model Criteria\
Because our goal is to have a correct model, we are choosing only explanatory variables    which have a significant relationship with angina. This model may be smaller than ones which have the goal of prediction. For the purposes of this class AIC or BIC are often used as model selection criteria as they penalize large models. AIC may overfit correct models and BIC penalizes large models even more, so we chose to focus on BIC as we imagine if somebody is trying to see if a person has angina  they will really need the  result to be as correct as possible. We choose the model that lowers BIC the most.

#### Subset Selection\
Subset selection does not evaluate all possible models; however, it is faster and does not cost as much because not all subsets are calculated. We chose to use Forwards Backwards Selection because we want a correct model which means having a smaller one. Of course, underfitting a model is not ideal however between underfitting and overfitting, underfitting the model is more likely to achieve the smaller and often more correct model which Forwards Selection or Forwards Backwards Selection is more likely to do. Forwards Backwards Selection does not underfit as much as Forwards Selection though, so we used Forwards Backwards as our subset selection. These were the models with the lowest BIC when looking at all possible models.
```{r, results = "hide",include=FALSE}
#used get best model fun from model_selection.R file
best.model.1982 = lm(formula = afatal_rate ~ location + miles + dry + jail + spirits, 
    data = fatality.1982)
best.model.1985 = lm(formula = afatal_rate ~ location + income + miles + jail, 
    data = fatality.1985)
best.model.1988 = lm(formula = afatal_rate ~ location + income + baptist, data = fatality.1988)
```

Model 1 : $$ Y_{1982} =  -0.7118 + 0.1313X_{region, other} + 0.1711X_{region, south} + 0.0002X_{miles} +0.0113X_{dry} + 0.2154X_{jail,yes} +  0.0888X_{spirits}$$

Model 2: $$ Y_{1985} =  0.56308 - 0.0094X_{region, other} + 0.0781X_{region, south} - 0.00004X_{income} + 0.00008X_{miles} + 0.13960X_{jail,yes}$$

Model 3: $$ Y_{1988} =  1.35883 - 0.01927X_{region, other} - 0.15759X_{region, south} - 0.00005X_{income} + 0.01255 X_{baptist}$$


### Diagnostics
With our “best” models of no interaction effects, we will perform diagnostics to ensure that it meets the assumptions of multiple regression, which are that observations are independent, errors have constant variance and they are normally distributed. We will look at different tests for these assumptions, and find and remove outliers. If necessary. we will transform data to correct for non-normality, non-linearity, or non-constant variance  

#### Testing for Normality
#####  QQ Plot
```{r, echo = FALSE}
plot_qq = function(M, y){
qqnorm(M$residuals, main = sprintf("QQPlot %s", y))
qqline(M$residuals)
}
par(mfrow = c(2,2))
plot_qq(best.model.1982, "Model 1")
plot_qq(best.model.1985, "Model 2")
plot_qq(best.model.1988, "Model 3")
```


This data does not seem to be normally distributed as there are heavy tails on all three models. There are outliers and the points towards the end of the line are not where the points would lie if the data was totally normally distributed. 

#### Shapiro Wilks Test
```{r, results = "hide",include=FALSE}
check.normality.assumptions = function(M){
  #qqnorm(M$residuals)
  #qqline(M$residuals)
  the.SWtest = shapiro.test(M$residuals)
  pValue = the.SWtest$p.value
  if (pValue < (0.01|0.05|0.1)){
    bc = boxcox(M,plotit = FALSE)
    lambda = bc$x[which.max(bc$y)]
    lambda #To see the value of lambda, transform to ln(y)
 
  results = list("p-value" = pValue, "lambda" = lambda)
  return(results)
  }
}
check.normality.assumptions(best.model.1982)
check.normality.assumptions(best.model.1985)
check.normality.assumptions(best.model.1988)
```


$H_{o}:$ errors is  normal   $H_{A}:$ errors is non- normal
  
   $P-Value_{model\ 1}:  0.2426$     $P-Value_{model\ 2}:  0.02156$     $P-Value_{model\ 3}:  0.0024$
  
The p-values for model 1 and 2 are greater than an alpha of 0.01. Thus we fail to reject the null and conclude that the errors are normally distributed. However, for model 3 the p-value is less than any significant alpha of 0.01, 0.1 and 0.05 thus we reject the null and conclude that the errors are not normally distributed.

#### Testing for constant variance
####  Errors vs Fitted Values
```{r, echo = FALSE}
plot_err_vs_fit = function(M,y){
plot(M$fitted.values, M$residuals, main = sprintf("Errors vs.Fitted Values %s", y), xlab = "Fitted Values",ylab = "Errors") 
abline(h = 0,col = "purple")
}
par(mfrow = c(2,2))
plot_err_vs_fit(best.model.1982, "Model 1")
plot_err_vs_fit(best.model.1985, "Model 2")
plot_err_vs_fit(best.model.1988, "Model 3")
```

There are many more data points on the lower values of X and a couple of outliers, but the data points are somewhat in a “band” around 0.


#### Breusch-Pagan Test
```{r,results = 'hide',include=FALSE}
bptest(best.model.1982)
bptest(best.model.1985)
bptest(best.model.1988)
```

$H_{o}:$ Residuals have equal variance   $H_{A}:$ Residuals do not have equal variance
 
   $P-Value_{model\ 1}$: 0.6729    $P-Value_{model\ 2}$: 0.1003      $P-Value_{model\ 2}$: 0.03305

All the models have a p-value greater than an alpha of 0.01 thus we fail to reeject the null and conclude that all the models have constant variance. 

#### Transformations for Model 3
In this section we wll transform the data of Model 3 to try to correct for non-normality in the errors using the Box-Cox transformation. Box-Cox transformations uses $(Y^\lambda−1)/\lambda$ to find the lambda that maximizes log-likelihood. We found that the lambda that maximizes the log- likelihood has a value o zero which corresponds to a log transformation. Once we transform the data, we will re-fit model 3.  We chose to use the transformed data for Model 3  because that one made the data most linear, most normal, and has the most constant variance compared to no transformation. Our next step was to look for and remove any outliers so that we could further meet the assumptions of linear regression. We did find 2 outliers in model 2 and two outliers in Model 3, when using an alpha of 0.05. We also checked for leverage points, which are data points which have a large influence on the regression line. When using cook’s distance  there were no leverage points, and so in the end we concluded to not remove any data points because the outliers are not influencing the models. 

```{r,  echo=FALSE}
transformData = function(data){
  data$afatal_rate = log(data$afatal_rate)
  return(data)
}
new.fatality.1988 = transformData(fatality.1988)
```
```{r,echo=FALSE, results="hide"}
new.best.model.1988 = lm(formula = afatal_rate ~ location + income + miles, data = new.fatality.1988)
shapiro.test(new.best.model.1988$residuals)
bptest(new.best.model.1988)

outliers = function(Model, data){
  ri = rstandard(Model)
  alpha = 0.1
  n = nrow(data) 
  p = length(Model$coefficients)
  cutoff = qt(1-alpha/(2*n), n - p )
  outliers = which(abs(ri) > cutoff)
  return(outliers)
}

outliers(new.best.model.1988, new.fatality.1988)
outliers(best.model.1982,fatality.1982)
outliers(best.model.1985,fatality.1985)

all.values = influence.measures(new.best.model.1988)$infmat
colnames(all.values)
p = new.best.model.1988$coefficients
n = nrow(new.best.model.1988)
lev.DF = which(all.values[,"cook.d"] >qf(0.50,p,n-p))

all.values = influence.measures(best.model.1985)$infmat
lev.DF = which(all.values[,"cook.d"] >qf(0.50,p,n-p))
lev.DF
```

#### Final Models
After verifying the assumptions and performing transformations, we can conclude that the assumptions of normality and constant variance are met for all models and no outliers need to be removed. Thus the best models are:

$$ Y_{1982} =  -0.7118 + 0.1313X_{region, other} + 0.1711X_{region, south} + 0.0002X_{miles} +0.0113X_{dry} + 0.2154X_{jail,yes} +  0.0888X_{spirits}$$


$$ Y_{1985} =  0.56308 - 0.0094X_{region, other} + 0.0781X_{region, south} - 0.00004X_{income} + 0.00008X_{miles} + 0.13960X_{jail,yes}$$

$$ ln(Y_{1988}) = -0.3367+ 0.0190X_{region, other} + 0.1032X_{region, south} - 0.0001X_{income} + 0.0001 X_{miles}$$

### Interpretation and Analysis 

#### A. Models

**(i)** Model 1: $\hat{\beta_{7}}:$ When spirit consumption increases by 1 bottle, the average number of alcohol fatalities increases by 1396 accidents holding all other variables constant

**(ii)** Model 3: $\hat{\beta_{3}}$ Souhtern states had  11087 more alcohol fatalities on average compared to  north eastern states, holding all other variables constant. In other words,    




# Sensitivity analysis 

#Causal Inference

# Discussion 

<span style='color:blue'> 
Conclude your analysis in this section. You can touch on the following topics. 
</span> 

- A brief recap of this project. 
- Findings in the inferential analysis interpreted in the context of Project STAR. 
- Suggestions for future research and/or policy making given your findings. 
- Caveats of the current analysis.

# Acknowledgement {-}

<span style='color:blue'>
By default, it is assumed that you have discussed this project with your teammates and instructors. List any other people that you have discussed this project with. 
</span>

# Reference {-}

Ruhm, C. J. (1996). Alcohol Policies and Highway Vehicle Fatalities. Journal of Health Economics, 15, 435–454.

Stock, J. H. and Watson, M. W. (2007). Introduction to Econometrics, 2nd ed. Boston: Addison Wesley.

[Beer taxation and Alcohol-related Traffic Fatalities](https://www.jstor.org/stable/1061141?casa_token=zFtQmEbeY_QAAAAA%3Af0JjUHvopvXG5WVl3ikDgWv863taJwuVu7kkAitlUq_bntLuMhLU7wJ8FQpv_Ajm2AOgLeZEoFDKbvuawzbt7SI5pc1w4xgPC1gcf_iIMwRNR6rXmw&seq=1#metadata_info_tab_contents ) This study explores the relationship between beer taxation and alcohol related Fatalities before 1996.

[The fatal toll of driving to drinkL the effect of minimal legal drinking age evasion on traffic fatalities](https://www.sciencedirect.com/science/article/pii/S016762960900112X?casa_token=RO1Xe8NlkFEAAAAA:q0IMnVgFl-6EUnHrFi9i1_eGHk3N72DKVBFxnIyYzq0VfdcRoYhkSTyjjIX9qrjz6LkUszEj) This covers the issues of teens drinking and driving with a really cool GIS approach that looks at regions. Discusses the evasion of laws.

[Drunk driving from an epidemiology perspective](https://www.researchgate.net/profile/James_Fell2/publication/242431276_Epidemiologic_Perspectives_on_Drunk_Driving/links/00b4953720449b1137000000.pdf) Published in the 80s, helps define the public health aspect of this question and conundrum. 

[Preventing Recidivism](https://www.sciencedirect.com/science/article/pii/S0047235297000755?casa_token=dYkm8bDafQYAAAAA:TvYyDbQ4SjB1OCIYKqp-AW8Dhr5OYSKp3aSmhLullPouRIimEjff3vFFJ7Lxyz4uxYOEiXMF)
More thoughts on non-jail ways that drunk driving can be curbed and fatalities lowered. A bit tangential.

[Relationship between gasoline prices and drunk driving crashes](https://www.sciencedirect.com/science/article/pii/S0001457510002423?casa_token=jpNEjTLZTRAAAAAA:jfG4LsqTxl3xxVb8DihHfrBd0wrACz9922VQxe4OR1QczbXWLlyn11I0F7YWM0w1rgPdvdz_) This could be interesting for discussion. It also contains some statistics about unemployment, which is why it was included.

[A historical review on drunk driving in the US](https://www.tandfonline.com/doi/full/10.1080/15389588.2012.656858?casa_token=nukMm9C_XSIAAAAA%3AbrRhZXddGjRjHEJXhjC6vXDTvdS1-THbcz1Af-geGmpLhRFflTGzo1dzQEX4CIj5OCWZywjsnTg) What has worked, and what hasn't? This will be an important contribution to the discussion section, as we can compare our results (Whatever they may be) to this comprehensive review.

# Session info {-}

<span style='color:blue'>
Report information of your `R` session for reproducibility. 
</span> 

```{r}
sessionInfo()
```
